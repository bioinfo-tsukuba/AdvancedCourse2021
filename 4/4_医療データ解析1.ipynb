{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_医療データ解析1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cNRFrWZE4Vp3",
        "j_7YnmsBVt6j"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5KD56pXSFsM"
      },
      "source": [
        "医療データ解析(１)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_CoktwrSGrc"
      },
      "source": [
        "# 概要\n",
        "* 医療データはcsvもしくはRDBのフォーマットで提供されることが多い\n",
        "* データ処理を効率的に行うため、それらのフォーマットのデータを処理する方法に触れる\n",
        "  * Pandas (pythonのライブラリ) の初歩的な操作に触れる\n",
        "  * Pandasの操作を踏まえて、リレーショナルデータベース (RDB) の初歩的な操作に触れる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJWyO80dB6hD"
      },
      "source": [
        "## データ処理知識の重要性\n",
        "データ解析を行う際に利用する大規模なデータ（例えば病院のカルテデータ、レセプトデータなど）は、大抵業務用データウェアハウス (DWH) から抽出し提供されています。そのデータベースの多くはリレーショナルデータベース (RDB) です。\n",
        "\n",
        "そのため、提供されるデータはcsvなど一般的なフォーマットの場合もあるが、データベースの出力ファイルを渡される場合が多い。その場合は、データにアクセスするためにはRDBへリストアしSQL言語などを用いて問い合わせる必要があります。\n",
        "\n",
        "実際の業務から得られたデータは、そのまま解析や機械学習に使えるほど綺麗には整えられてはいません。\n",
        "\n",
        "データ分析において、データサイエンティストが最も時間を費やし、かつ最も楽しめない業務としてデータの整理やノイズや誤字などを除去し綺麗にするデータクリーニング（クレンジング）作業が挙げられています（2017 Data Scientist Report, CrowdFlower）。\n",
        "\n",
        "このクリーニング作業は、どのようなデータが誤っているのかを見分けることができなければならないため、作業者がその分野の知識を有していなければならない。\n",
        "\n",
        "医療データは要配慮個人情報であり、多くの場合保管やアクセスの条件が厳しいため、外部の力を借りるハードルがやや高く、そのため研究者のチーム内でデータ処理の多くの部分を賄う必要があります。\n",
        "\n",
        "提供されたデータを処理するとき、巨大なデータの扱い方を知っているか知らないかでは処理時間に大きな差が出てきます。\n",
        "\n",
        "発展著しい医療情報分野において、研究の生産性を高めるためにもデータ処理の知識が重要となります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNRFrWZE4Vp3"
      },
      "source": [
        "## Pandas (Python)\n",
        "Pandasはpythonの外部ライブラリでも著名なものの一つであり、大規模なデータセットを処理するための便利な機能を提供しています。\n",
        "\n",
        "listやdictなどの組み込み型を駆使して行うデータ操作よりも、比較的人間の直感に近い操作を提供しています。\n",
        "\n",
        "使用者が多くWeb上にドキュメントやヘルプも豊富なので、やりたいことを実現するコードが既に誰かが公開していたり、エラーに直面してもエラーコードを検索すれば解決法やヒントが見つかりやすいので、何はともあれpythonでデータ処理を行うのであればpandasを用いることになると思います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPhrV3UFSOQA"
      },
      "source": [
        "## リレーショナルデータベース (RDB)\n",
        "データベースとは、複数による共同利用を意図して、データを組織的に、効率的に、永続的に蓄積し整理がなされたデータ群のことであり、これを管理することを目的としたシステムがデータベースシステムです。\n",
        "\n",
        "複数人からのアクセスを意図しているため、データベースシステムは、データの持ち方やデータの操作の手続きというものがあらかじめ規定されています（データモデル）。使用者とデータベースシステムの間に立ち、使用者の指示に対してその規定に従うような具体的な操作を行うことによって、両者の仲立ちを行うのがデータベースマネジメントシステム（DBMS）です。\n",
        "\n",
        "現在世の中によく普及しているのが、データモデルとしてリレーション（関係）を採用し（しばしば例えに表が持ち出されます）、データベースをリレーションの集合とみなすリレーショナルデータベース (RDB, もしくは関係データベース）で、その管理システムがリレーショナルデータベースマネジメントシステム (RDBMS)です。RDBの設計にはリレーションの正規化などの概念があります（この演習では触れませんが、データベース設計などに興味がある方は、「データベースシステム(北川博之)」、「リレーショナルデータベース入門 (増永良文)」などを手に取ってみてください）。\n",
        "\n",
        "多くの業務システムはRDBを採用しています。そのため、自治体や医療機関から提供される医療データもまたそれに準じた形式であることが多いです。\n",
        "\n",
        "この時間は、SQLiteというRDBMSを用いてRDBの操作方法についても触れます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtPwXJQTYQw1"
      },
      "source": [
        "# 基本操作とその比較\n",
        "\n",
        "以下のオープンデータを用いて操作の基本を学びます。\n",
        "ブラウザからダウンロードするか、colabのセルに以下のコマンドを打ち込んでデータをダウンロードしてください。回線が細いのか数十秒かかります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeMR7pvIi_G5"
      },
      "source": [
        "https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide\n",
        "よりcsvを取得してみましょう。\n",
        "\n",
        "https://opendata.ecdc.europa.eu/covid19/casedistribution/csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixHkle3UjHCC"
      },
      "source": [
        "!wget -c https://opendata.ecdc.europa.eu/covid19/casedistribution/csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pXfDbuaqyi1"
      },
      "source": [
        "ファイルの内容を確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbPIS7PT2-2p"
      },
      "source": [
        "%%bash\n",
        "# ファイル名がcsvのみで誤解を招きやすので名前変更\n",
        "mv csv eu.csv\n",
        "# ファイルの内容確認\n",
        "head -n 10 eu.csv\n",
        "# ファイルの行数確認\n",
        "wc -l eu.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHoODN0b7YA"
      },
      "source": [
        "## Pythonの基本機能の場合\n",
        "\n",
        "このcsvファイルを操作する方法について見ていきます。\n",
        "\n",
        "まず、pythonの基本的な機能だけでcsvを操作してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFO3SV1Mj8XS"
      },
      "source": [
        "# csvの読み込み\n",
        "import csv\n",
        "\n",
        "with open('eu.csv', 'r') as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  # ヘッダ行\n",
        "  header = next(csv_reader)\n",
        "  display(header)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKdlL-wSsVFb"
      },
      "source": [
        "csvファイルは先頭に各列の名称を記録するヘッダ行が存在する場合があります。これは1行とは限らず、複数行コメント行が続くこともあるため、確認する必要があります。\n",
        "\n",
        "今回はヘッダ行1行のみであることがわかったため、読み込んだ直後に１行をヘッダとして読み取ります。\n",
        "その後、残りの行をforループで処理することによって各行への処理を行います（下の例は1ループ目でbreakすることで中断しています）。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lusWaW0yXxQ4"
      },
      "source": [
        "# 各行を処理する場合\n",
        "with open('eu.csv', 'r') as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  header = next(csv_reader)\n",
        "  \n",
        "  for row in csv.DictReader(f, header):\n",
        "    # 以下、行毎の処理を記述\n",
        "    print('----Row----')\n",
        "    print(row) # rowに各行の内容が辞書型で入っている\n",
        "    print('----')\n",
        "    print(row['dateRep']) # 各列の名称でアクセス\n",
        "    print(row['deaths'])\n",
        "    print(row['countriesAndTerritories'])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghXU7al7wXa7"
      },
      "source": [
        "Jupyter notebookではマジックコマンドとして`%time`を行頭に付けるとその行の実行時間を表示してくれます。\n",
        "\n",
        "Google colabではセルの左側にセル全体での所要時間が表示されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbEFnD6mwbTP"
      },
      "source": [
        "import time\n",
        "with open('eu.csv', 'r') as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  header = next(csv_reader)\n",
        "  %time time.sleep(2)\n",
        "  for row in csv.DictReader(f, header):\n",
        "    print('----Row----')\n",
        "    print(row) # rowに各行の内容が辞書型で入っている\n",
        "    print('----')\n",
        "    print(row['dateRep']) # 各列の名称でアクセス\n",
        "    print(row['deaths'])\n",
        "    print(row['countriesAndTerritories'])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TAlxb1PUENu"
      },
      "source": [
        "%%prun\n",
        "import time\n",
        "with open('eu.csv', 'r') as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  header = next(csv_reader)\n",
        "  %time time.sleep(2)\n",
        "  for row in csv.DictReader(f, header):\n",
        "    print('----Row----')\n",
        "    print(row) # rowに各行の内容が辞書型で入っている\n",
        "    print('----')\n",
        "    print(row['dateRep']) # 各列の名称でアクセス\n",
        "    print(row['deaths'])\n",
        "    print(row['countriesAndTerritories'])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vmhf8l3QZADZ"
      },
      "source": [
        "上の例ではcsvファイルの読み込みと、time.sleep(2)関数の時間を計測しています。後者では指示通りに2秒待機しているので2番目の計測結果のWall (clock) timeが2 sになっています。\n",
        "\n",
        "セルの頭にマジックコマンドとして`%%time`をつけるとセルの実行時間を表示してくれます。\n",
        "\n",
        "次の例として、csvファイルの'cases'列の要素の合計値を計算し、その実行時間を計測してみましょう。\n",
        "\n",
        "次の例では単純に1行ずつ読み込み、cases列の値を取得して、加算するという方法をとっています。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKwBvTjvxdV6"
      },
      "source": [
        "%%time\n",
        "with open('eu.csv', 'r') as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  header = next(csv_reader)\n",
        "  sum_cases = 0\n",
        "  for row in csv.DictReader(f, header):\n",
        "    sum_cases = sum_cases + row['cases']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmitKT32yg9g"
      },
      "source": [
        "おそらくTypeErrorが起きたと思います。これはこの読み取り方法だとcsvファイルの各要素はstr (文字列) として扱うためで、int (整数型) の計算はできないからです。\n",
        "\n",
        "なおエラーメッセージの最下段に'Search stack overflow'というポップが出ているともいます。これをクリックすると、このエラーメッセージについてstackoverflowというコミュニティサイトに限定して検索した結果に飛ぶので、問題解決のために有用です。\n",
        "\n",
        "ここでは解決のために、strをintとして扱うためにint(文字列)のようにします。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY5jJglhy-QO"
      },
      "source": [
        "%%time\n",
        "with open('eu.csv', 'r') as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  header = next(csv_reader)\n",
        "  sum_cases = 0\n",
        "  for row in csv.DictReader(f, header):\n",
        "    sum_cases = sum_cases + int(row['cases'])\n",
        "  display(sum_cases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t6rQJGR15Ak"
      },
      "source": [
        "この例では約6万行の処理に345 msかかったとわかります。\n",
        "\n",
        "続いてもう一つ別の列も処理してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er78P1rZ4RaH"
      },
      "source": [
        "with open('eu.csv', 'r') as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  header = next(csv_reader)\n",
        "\n",
        "  sum_cases = 0\n",
        "  sum_deaths = 0\n",
        "  for row in csv.DictReader(f, header):\n",
        "    sum_cases = sum_cases + int(row['cases'])\n",
        "  for row in csv.DictReader(f, header):\n",
        "    sum_deaths = sum_deaths + int(row['deaths'])\n",
        "\n",
        "  display(sum_cases)\n",
        "  display(sum_deaths)\n",
        "  # pythonではforは次のようにリスト内包表記でも書くことができ、こちらの方が処理速度があがることが多い"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTo7-XXL5P1B"
      },
      "source": [
        "冒頭でファイルの頭数行を確認した通り確かにこの列には数値が入力されていましたが、合計値がおかしいことになっています。\n",
        "\n",
        "これは'cases'のループでファイルを終わりまで読み込んでしまい、'deaths'のループでは何も読み込むことができなかったためです。\n",
        "\n",
        "これを解決するためには、ファイルポインタの位置をseekメソッドによってファイルの先頭に戻す必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g2AaIAaZYok"
      },
      "source": [
        "with open('eu.csv', 'r') as f: # ここでファイルハンドルを取得しています\n",
        "  csv_reader = csv.reader(f)\n",
        "  header = next(csv_reader)\n",
        "\n",
        "  sum_cases = 0\n",
        "  sum_deaths = 0\n",
        "  for row in csv.DictReader(f, header):\n",
        "    sum_cases = sum_cases + int(row['cases'])\n",
        "\n",
        "  f.seek(0) # ファイルポインタの位置を先頭に戻します\n",
        "  header = next(csv_reader) # 先頭に戻ったのでヘッダ行を読むか、もしくは既に上で確保しているので単にnext(csv_reader)として捨てます。\n",
        "  for row in csv.DictReader(f, header):\n",
        "    sum_deaths = sum_deaths + int(row['deaths'])\n",
        "\n",
        "  display(sum_cases)\n",
        "  display(sum_deaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OibC4nn47omq"
      },
      "source": [
        "もしくは、単純に同じループの中で処理するという手もあります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8lTAskHZjE_"
      },
      "source": [
        "with open('eu.csv', 'r') as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  header = next(csv_reader)\n",
        "\n",
        "  sum_cases = 0\n",
        "  sum_deaths = 0\n",
        "  for row in csv.DictReader(f, header):\n",
        "    sum_cases = sum_cases + int(row['cases'])\n",
        "    sum_deaths = sum_deaths + int(row['deaths'])\n",
        "  display(sum_cases)\n",
        "  display(sum_deaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGAoUWCS70uH"
      },
      "source": [
        "いずれにせよ、ファイルを上から1行ごとに処理していく、というのは手間です。\n",
        "最初にcsvファイルを走査して、各行入ったリストを作成しそれを操作するというのも見通しが悪く、ファイルポインタというコンピュータシステム側に近い動作を意識しながら操作するというのも、本来やりたいこととは別の作業にリソースを取られている感じがします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkkTYzEktYK-"
      },
      "source": [
        "## pandasの場合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKIIx_8FhJTV"
      },
      "source": [
        "では続いてpadasによる操作を行なってみます。pandasでは、もう少し抽象的な操作が可能になります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvZndfZScLGM"
      },
      "source": [
        "# colabではデフォルトで使用できます　自分のPCなどではpip install pandasなどとしてインストールする必要があります\n",
        "# 使用頻度が高いのでpandasといちいち打つのは手間です　なのでpdで使えるようにします\n",
        "import pandas as pd \n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXNj7JYFfF_a"
      },
      "source": [
        "eu = pd.read_csv('eu.csv', header=0) # ヘッダ行の有無や位置を指定することができます　しなくてもうまく対応してくれることがあります"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76stGDkcILBC"
      },
      "source": [
        "display(eu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOx555IeIzyy"
      },
      "source": [
        "列名は日本語や記号も用いることができます。しかし、列名（ラベル）を指定する操作で、うまく認識されないことがあるため、できるだけ#, @, &, $, !などの記号は避け、英数字とアンダースコアを用いると良いと思います。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPXSDM0IEfN8"
      },
      "source": [
        "### データフレームの情報を表示する\n",
        "pandas.DataFrameの情報を表示するためのメソッドがいくつか存在します。\n",
        "データフレームの形状(行数・列数)というった一般的な情報や、要素の型をどう認識しているか、使用メモリ、各列の統計情報などを表示させることができます。\n",
        "\n",
        "データを概観する事は、各列のカーディナリティ（列の値の種類）、欠損値の有無など分析やデータクリーニングなどを行う上でも重要な情報を与えてくれるので、データを入手したらまず行なってみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS2s8Xt2Fnpl"
      },
      "source": [
        "DataFrame.info()メソッドはDataFrameの形状や、各列のラベル、非欠損値のカウント情報、pandasが認識している各列のデータのタイプなどの情報を表示することができる。\n",
        "\n",
        "これによって、例えばcsvファイルではIDとして542など文字列として格納されていたものが542.0など浮動小数点型 (float) として扱われてしまい、文字列のつもりで操作するとエラーを吐くといった状況になります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45QwLbA4EcfW"
      },
      "source": [
        "eu.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB_-X4fcX5rV"
      },
      "source": [
        "#### Google colab環境のスペック\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gA9xwoxXsLi"
      },
      "source": [
        "!df -h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-kAN3rKXxN5"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPGiAlooXss9"
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01pu8YM2Gq7-"
      },
      "source": [
        "ここで実際のcsvファイルの先頭をもう一度チェックしてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMEcyn6AGlPX"
      },
      "source": [
        "!head -n 3 eu.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1zJi4xRZR4s"
      },
      "source": [
        "eu.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiEttlCAGp44"
      },
      "source": [
        "popData2019が浮動小数点型として扱われるべきかは迷うところです。\n",
        "\n",
        "次にDataFrame.describe()メソッドを試してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIxm6ZKSHMmW"
      },
      "source": [
        "eu.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6j4KEErHZfn"
      },
      "source": [
        "describe()メソッドは各列についての簡単な統計情報を表示してくれます。\n",
        "\n",
        "popDataの値が大きいため指数表記になっています。pandasの表示についての設定を変更してみましょう。\n",
        "\n",
        "推奨値などはないため、適当に状況にあわせて変更してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFSzYXkWH0LY"
      },
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format # 項目を直接指定し変更する場合\n",
        "pd.set_option('display.max_columns', 50) # 正規表現で項目を検索し、マッチしたものを変更する場合\n",
        "pd.set_option('display.width', 150)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "pd.options.display.max_rows = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuMcrs2kIl9I"
      },
      "source": [
        "eu.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoAX4GOVJqka"
      },
      "source": [
        "casesおよびdeathsのminがマイナスになっているのが気になります。時間があれば各自確認してみてください。\n",
        "\n",
        "続いてmissingnoライブラリを用いて、NaN (非値)の分布状況を可視化してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04sPZICuOKSm"
      },
      "source": [
        "import missingno as msno\n",
        "msno.matrix(df=eu, filter='bottom', figsize=(12,9), color=(0.5,0,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVp0jQ4KOFUM"
      },
      "source": [
        "### データ操作"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LB8Mkz9EZZb"
      },
      "source": [
        "ではcsvの時と同じように、各行、各列の操作をしてみます。\n",
        "\n",
        "pandasはさまざまな方法で表の要素にアクセスできるため混乱しやすいです。基本的にはlocを知っていれば大体の事は足ります。\n",
        "\n",
        "| | インデックスによるアクセス | ラベルによるアクセス |\n",
        "|--|:--:|:--:|\n",
        "| 単一要素アクセス| iat  | at |\n",
        "| 複数要素アクセス| iloc | loc |\n",
        "\n",
        "その他、eu[0:1]のような'スライス'表記によってもアクセスできます。\n",
        "\n",
        "start:stop:stepを意味します。step=1はしばしば省略されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Y1Wsbxb6gr"
      },
      "source": [
        "#　スライス\n",
        "a = [0, 1, 2, 3, 4, 5]\n",
        "display(a[0:1])\n",
        "display(a[1:2])\n",
        "display(a[0:2])\n",
        "display(a[:2])\n",
        "display(a[0:6:1])\n",
        "display(a[0:6:2])\n",
        "display(a[:6:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pwAHaAnSyUj"
      },
      "source": [
        "# 1つまでであればdisplayなしでその中身が表示できる\n",
        "eu.iat[0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpAK_7tqTcM3"
      },
      "source": [
        "eu.at[0, 'day'] # 行のラベルが数値で、結果的にインデックスと同じになっている"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiwcE_k-rtGL"
      },
      "source": [
        "# 1行目の指定\n",
        "r0 = eu[0:1]\n",
        "display(r0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7diI9EDJyTb"
      },
      "source": [
        "# 複数行の指定\n",
        "eu[0:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4mmGF7aNboC"
      },
      "source": [
        "# 以下の式は同じ結果を返します\n",
        "display(eu.loc[[0, 1], :])\n",
        "display(eu.loc[[0, 1], ])\n",
        "display(eu.loc[[0, 1]])\n",
        "display(eu[0:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1dRiZUK-Wej"
      },
      "source": [
        "%%time\n",
        "# 列の指定\n",
        "a = eu[['cases']]\n",
        "display(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYm0Elr_rbSF"
      },
      "source": [
        "%%time\n",
        "# 列の指定\n",
        "eu[['cases', 'deaths']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdrzVsfba60t"
      },
      "source": [
        "# 列の指定(loc)\n",
        "eu.loc[:, ['cases', 'deaths']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GETFO89sjU2"
      },
      "source": [
        "# 複数行、複数列の指定\n",
        "display(eu.loc[3:5, ['cases']]) # 3:5はラベル扱い\n",
        "display(eu.iloc[3:6, 4:5]) # 3:6はインデックス番号のスライス扱い\n",
        "display(eu.iloc[3:6, 4:6]) # 3:6はインデックス番号のスライス扱い"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLnaEut8-3JG"
      },
      "source": [
        "%%time\n",
        "# 特定の条件を持つレコードの検索　pandasが読み込みの際にcasesを整数型として自動的に設定したので大小比較が可能\n",
        "eu.query(' cases < 60 ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfF-rcVGPxXS"
      },
      "source": [
        "# NaNを持つ行、列の削除\n",
        "# このデータに対して妥当な操作かはさておき、DataFrameにはNaNを持つ行もしくは列を簡単に削除するメソッドが存在する\n",
        "eu_drop = eu.dropna(how='any', axis='rows')\n",
        "# 削除後再確認してみる\n",
        "msno.matrix(df=eu_drop, filter='bottom', figsize=(12,9), color=(0.5,0,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXln09nxtl6x"
      },
      "source": [
        "## SQLite (RDB) の場合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzdDlVilURdq"
      },
      "source": [
        "RDBを操作するためには、SQL言語を使います。SQL言語はなるべくコマンド文を英文に似せるように設計されています。Google colab.はSQLの予測補完にも対応しているので活用してください。\n",
        "\n",
        "pythonにはsqliteライブラリが入っており、colabのjupyter notebookではsqlコマンドをセルから実行することができます (自前の環境ではipython-sqlやsqlalchemyライブラリをインストールする必要があるかもしれません)。\n",
        "\n",
        "マジックコマンドとして%sqlを行頭につけるとsqlコマンドとして解釈されます。\n",
        "\n",
        "%%sql だとセル全体がsqlとして解釈されます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUQLw44A-EgT"
      },
      "source": [
        "### CSVのインポート、テーブルの作成、レコードのインサート\n",
        "操作のテストとして、先のcsvファイルをもとにSQLite3を用いてテーブル（表）を作成してみましょう。\n",
        "\n",
        "*sqlite3のドットコマンドが利用できれば.importでcsvを簡単にインポートすることができるのですが、ここではcsvからインサートする手段を取ります。*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-VghPaUuPBn"
      },
      "source": [
        "%load_ext sql\n",
        "# 接続先データベースとしてtmp.sqlite3ファイルを指定しています　存在しない場合は新規作成されます。\n",
        "%sql sqlite:///tmp.sqlite3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EctqT7ZkmCVg"
      },
      "source": [
        "#### テーブルの作成\n",
        "まずcsvの内容を受け取るテーブルを作ります。\n",
        "```\n",
        "CREATE TABLE テーブル名 (列名1 型, 列名2 型, ...);  # sql文は;で文の終わりを示します。\n",
        "```\n",
        "のようなコマンドでテーブルを作成できます。\n",
        "'テーブル名'という名前のテーブルを作り、列は'列名1', '列名2', ...、という意味です。型はその列に格納されるべき型を制限します。例えば整数型（INTEGER）が設定されている列に文字列をインサートするような操作の場合、整数型に変換を試みたり、エラーが返されたります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy9trbWcoSzm"
      },
      "source": [
        "\n",
        "格納できる型が限定されていることは様々なメリットをもたらしますが、データの整合性を保つことに有用なのもその一つです。大小比較することが明白な列などはあらかじめ数値の型を指定しておいた方が良いでしょう。\n",
        "\n",
        "どのような型が使えるのか、違反があった場合は、変換するのか、エラーを返すのか、無視するのかなどはシステムによって異なります。SQLの規定に従っていればどのように実現するように設計するかは自由なので、基本操作にはまず違いはありませんが、SQLにも言うなれば方言があります。今回使っているSQliteのSQLと、他の有名なRDBMS、例えばPostgreSQLのSQL, MariaDBのSQL、MicrosoftのSQL serverのSQLは微妙に文法などが異なるので、違うシステムに対してはコマンドに互換性があるかを確認する必要があります。\n",
        "\n",
        "SQLiteでは、テーブルを作るときに列に型を指定した場合、違反する型がインサートされようとした場合、まず変換を試みますが、変換できない場合はそのまま格納するという動作になります。\n",
        "\n",
        "SQliteではTEXT型（文字列）、INTEGER型（整数）、REAL型（浮動小数点）、NUMERIC型（整数または浮動小数点）、NONE（変換しない）を指定することができます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7AFnSR5obqp"
      },
      "source": [
        "\n",
        "他にもプライマリキー制約やユニーク制約、NOT NULL制約等々といった、列に格納される値にさらに制限をかけることもここで行います。後程実際にその効果を確かめるコマンドを実行してみましょう。\n",
        "\n",
        "\n",
        "SQLでは大文字小文字を区別しません。そのためどちらを使うのも自由ですが、基本的には視認性の観点からSQLコマンドは大文字、列名などは小文字にするのが一般的です。\n",
        "\n",
        "```\n",
        "CREATE TABLE test (a TEXT, b INTEGER, c TEXT)\n",
        "```\n",
        "\n",
        "列名には文字、数字、アンダースコアを用いることができますがハイフン '-' などを含めることはできません。その場合は列名をクォーテーションなどで括る必要があります。しかしクォーテーションの存在は面倒な事態を引き起こすことが多いため、省略するか別の文字で置換しておくことが無難でしょう。\n",
        "\n",
        "今回は_に置き換えることにします。\n",
        "\n",
        "では、列名を決めるためにcsvのヘッダ行を再確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZQ83jfNmcma"
      },
      "source": [
        "!head -n 1 eu.csv\n",
        "# 最後の列のCOVID-19にハイフンが含まれているので次のセルではアンダースコアに置換"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqt94NGLl2Zw"
      },
      "source": [
        "%%sql\n",
        "CREATE TABLE eu (dateRep,day,month,year,cases,deaths,countriesAndTerritories,geoId,countryterritoryCode,popData2019,continentExp,Cumulative_number_for_14_days_of_COVID_19_cases_per_100000);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heuNbEQbnAnb"
      },
      "source": [
        "# 作成したテーブルが気に食わない、ミスしたなどで削除したい場合は\n",
        "%sql DROP TABLE eu;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcF11fCSxzTr"
      },
      "source": [
        "続いてcsvファイルの各行のデータから、テーブルへレコードをインサートするためのSQLクエリを生成します。レコードは1つずつインサートすることもできるのですが、まとめてインサートした方がトランザクションの関係上早く済みます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RKBRcHAWm5R"
      },
      "source": [
        "#### レコードのインサート\n",
        "テーブルにレコード（行）を記録するには、\n",
        "```\n",
        "INSERT INTO テーブル名 (列名1, 列名2, ...) VALUES (0, 1, 2, ...);\n",
        "```\n",
        "というコマンドを用います（テーブル名の後の列名は条件によっては省略できますがおすすめしません。テーブルの構造は後から変えることができるため、常にそのコマンドが意図した通りに実行されるとは限らないためです）。\n",
        "```\n",
        "INSERT INTO テーブル名 (列名1, 列名2, ...) VALUES (0, 1, 2, ...), (A, B, C, ...);\n",
        "```\n",
        "のように複数のレコードを指定することもできます。\n",
        "\n",
        "colab上でレコードを作成しインサートするには、SQLクエリを直接文字列として作成したり、まずSQLクエリの枠だけ用意し、VALUES以降の値については？（プレースホルダ）としておき、後から実際に実行する時点で変数を渡す、などの方法があります。\n",
        "\n",
        "早速試してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a72dSEbj04p-"
      },
      "source": [
        "%%time\n",
        "# pandasで作成する場合\n",
        "eu = pd.read_csv('eu.csv', header=0)\n",
        "tuples_pd = [tuple(row) for _i, row in eu.iterrows()]\n",
        "\n",
        "display(len(tuples_pd))\n",
        "display('-----')\n",
        "display(tuples_pd[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwS--Xfo5e90"
      },
      "source": [
        "%%time\n",
        "# pandasで作成する場合2\n",
        "eu = pd.read_csv('eu.csv', header=0)\n",
        "tuples_pd2 = eu.apply(tuple, axis=1)\n",
        "\n",
        "display(len(tuples_pd2))\n",
        "display(tuples_pd2[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Yi-3NG2F1R"
      },
      "source": [
        "%%time\n",
        "with open('eu.csv', 'r') as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  header = next(csv_reader)\n",
        "  tuples_csv = [tuple(row) for row in csv_reader]\n",
        "\n",
        "  display(len(tuples_csv))\n",
        "  display(tuples_csv[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnxlRA1m7OCb"
      },
      "source": [
        "# マジックコマンドで文字列のクエリを作りインサート\n",
        "# 複数のレコードをインサートするように指示できますが、このデータを全て結合すると400万文字以上になるため、入力を受け付けてくれないでしょう。\n",
        "# そこで効率は良くないですが１レコードずつインサートしてみます\n",
        "# 問題点としては、文字列はクォーテーションで括らなければならず、もし元のデータにクォーテーションもしくはダブルクォーテーションが含まれていた場合は区別のために\\を直前につける必要があることです（エスケープ）"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3VmHww4hzmY"
      },
      "source": [
        "num_of_dquot = !fgrep \"\\\"\" eu.csv"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI7J3ffHmGM9"
      },
      "source": [
        "len(num_of_dquot)\n",
        "# 257件あるらしいことがわかる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF6ndgtxnWTh"
      },
      "source": [
        "num_of_dquot # 内容のチェック　csvの区切り文字であるコンマを含む文字列があるための模様 この形式であれば上の操作をした時に除去されているため無視できる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJNKgIi3ldvz"
      },
      "source": [
        "%%bash\n",
        "fgrep \"\\'\" eu.csv | head\n",
        "# シングルクォートはないらしい"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix9_y4cs6wtV"
      },
      "source": [
        "# 文字列加工が望んだ結果になっているかをチェック\n",
        "_t = tuples_csv[0]\n",
        "quoted = ['\\\"' + x + '\\\"' for x in _t] # 各列の要素を\"で囲む\n",
        "_val = ','.join(quoted) # X, Y, Zのように繋げた文字列にする\n",
        "display(_val)\n",
        "cmd = 'INSERT INTO eu (dateRep,day,month,year,cases,deaths,countriesAndTerritories,geoId,countryterritoryCode,popData2019,continentExp,Cumulative_number_for_14_days_of_COVID_19_cases_per_100000) VALUES (' + _val + ')'\n",
        "display(cmd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS87-agcdkrn"
      },
      "source": [
        "%%capture\n",
        "for _t in tuples_csv[0:10000]: # 時間がかかるので10000件だけ 90秒ほど　出力が多いのでcaptureマジックコマンドを利用して抑制\n",
        "  quoted = ['\\\"' + x + '\\\"' for x in _t] # 各列の要素を\"で囲む\n",
        "  _val = ','.join(quoted) # X, Y, Zのように繋げた文字列にする\n",
        "  cmd = 'INSERT INTO eu (dateRep,day,month,year,cases,deaths,countriesAndTerritories,geoId,countryterritoryCode,popData2019,continentExp,Cumulative_number_for_14_days_of_COVID_19_cases_per_100000) VALUES (' + _val + ')'\n",
        "  %sql $cmd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOSBUJPdZrKK"
      },
      "source": [
        "変数を利用した場合を試してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSUI3sbct2WE"
      },
      "source": [
        "%%capture\n",
        "%sql DELETE FROM eu;\n",
        "for _t in tuples_csv[0:1000]: # 1000件だけ　12秒ほど\n",
        "  dateRep,day,month,year,cases,deaths,countriesAndTerritories,geoId,countryterritoryCode,popData2019,continentExp,Cumulative = _t\n",
        "  %sql INSERT INTO eu (dateRep,day,month,year,cases,deaths,countriesAndTerritories,geoId,countryterritoryCode,popData2019,continentExp,Cumulative_number_for_14_days_of_COVID_19_cases_per_100000) VALUES \\\n",
        "  (:dateRep,:day,:month,:year,:cases,:deaths,:countriesAndTerritories,:geoId,:countryterritoryCode,:popData2019,:continentExp,:Cumulative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPhuC3y0scML"
      },
      "source": [
        "変数を利用した方が楽に記述できそうですが、若干時間がかかるようです。\n",
        "\n",
        "6万件のレコードをインサートするには12x60秒ほどかかると思われます。\n",
        "\n",
        "#### バルクインサート\n",
        "続いてバルクインサートを試してみましょう。バルクインサートも、先にSQL文を用意しておきますが、後から?に対応した値を格納したタプルをリストなどにしてまとめて渡します。\n",
        "```\n",
        "a = [0,1,2,3] # 配列\n",
        "b = (0,1,2,3) # タプル　後から内容を変えることができない（イミュータブル）\n",
        "```\n",
        "イメージとしてはまさに先に挙げたインサートのようになります。\n",
        "```\n",
        "INSERT INTO テーブル名 (列名1, 列名2, ...) VALUES (0, 1, 2, ...);\n",
        "\n",
        "INSERT INTO テーブル名 (列名1, 列名2, ...) VALUES (0, 1, 2, ...), (A, B, C, ...);\n",
        "```\n",
        "バルクインサートはマジックコマンド%sqlを用いた場合では挙動が不明なため、sqlite3ライブラリを用いて試します。これは1トランザクションでレコードのインサートを試みるため、先のループでインサートを繰り返すよりも高速化が見込めます。実行時間を比較するため、sqlite3ライブラリを用いて、1レコードずつforループでインサートするのも試してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kADBiGTW4mlc"
      },
      "source": [
        "# データベースを空に\n",
        "%sql DELETE FROM eu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IXqxFSo7vRO"
      },
      "source": [
        "%%time\n",
        "# 1000レコードのみ試す\n",
        "import sqlite3\n",
        "\n",
        "con = sqlite3.connect('tmp.sqlite3')\n",
        "cur = con.cursor()\n",
        "\n",
        "cmd_create = 'CREATE TABLE IF NOT EXISTS eu (' \\\n",
        "  'dateRep TEXT,' \\\n",
        "  'day INTEGER,' \\\n",
        "  'month INTEGER,' \\\n",
        "  'year INTEGER,' \\\n",
        "  'cases INTEGER,' \\\n",
        "  'deaths INTEGER,' \\\n",
        "  'countriesAndTerritories TEXT,' \\\n",
        "  'geoId TEXT,' \\\n",
        "  'countryterritoryCode TEXT,'\\\n",
        "  'popData2019 TEXT,'\\\n",
        "  'continentExp TEXT,'\\\n",
        "  'Cumulative_number_for_14_days_of_COVID_19_cases_per_100000 REAL)'\n",
        "\n",
        "cur.execute(cmd_create)\n",
        "for _t in tuples_csv[0:1000]:\n",
        "  cur.execute(\n",
        "    'INSERT INTO eu (dateRep,day,month,year,cases,deaths,countriesAndTerritories,geoId,countryterritoryCode,popData2019,continentExp,Cumulative_number_for_14_days_of_COVID_19_cases_per_100000) '  \\\n",
        "    ' VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', _t\n",
        "  )\n",
        "  con.commit()\n",
        "con.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0JbN8CW8oMo"
      },
      "source": [
        "7秒ほどかかりました。次にバルクインサートを試してみましょう。\n",
        "\n",
        "forループで各タプルを渡す代わりに、タプルをリストでまとめているtuples_csvを直接渡しています（メソッドもexecuteからバルクインサート用のexecutemanyに変わっています）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znRlUhQA8Pc8"
      },
      "source": [
        "! rm tmp.sqlite3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P3AXCVkwds3"
      },
      "source": [
        "%%time\n",
        "import sqlite3\n",
        "con = sqlite3.connect('tmp.sqlite3')\n",
        "cur = con.cursor()\n",
        "\n",
        "cmd_create = ('CREATE TABLE IF NOT EXISTS eu ('\n",
        "  'dateRep TEXT,' \n",
        "  'day INTEGER,' \n",
        "  'month INTEGER,' \n",
        "  'year INTEGER,' \n",
        "  'cases INTEGER,' \n",
        "  'deaths INTEGER,' \n",
        "  'countriesAndTerritories TEXT,' \n",
        "  'geoId TEXT,' \n",
        "  'countryterritoryCode TEXT,'\n",
        "  'popData2019 TEXT,'\n",
        "  'continentExp TEXT,'\n",
        "  'Cumulative_number_for_14_days_of_COVID_19_cases_per_100000 REAL)')\n",
        "cur.execute(cmd_create)\n",
        "cur.executemany(\n",
        "    'INSERT INTO eu (dateRep,day,month,year,cases,deaths,countriesAndTerritories,geoId,countryterritoryCode,popData2019,continentExp,Cumulative_number_for_14_days_of_COVID_19_cases_per_100000) ' + \\\n",
        "    ' VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', tuples_csv\n",
        ")\n",
        "\n",
        "con.commit()\n",
        "con.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvtNPGfjL3xD"
      },
      "source": [
        "# どのようなテーブルが存在するか、どのようなコマンドで作られたかを確認する\n",
        "%sql select * from sqlite_master;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUn5w0Ltm7Hs"
      },
      "source": [
        "### SQLによるデータ操作\n",
        "それではテストデータが用意できたところでSQL文を用いてデータの操作をしてみましょう。\n",
        "\n",
        "RDBの特徴として、行の順序や列の順序を重要視しないというものがあります。\n",
        "\n",
        "そのため、X列Y行目という指定方法はありません。一応、テーブル定義の際の列の順序、インサートしたレコードの順序が、結果的に保たれている場合はありますが、それは保証されている動作ではないので、それを期待したコードは書かないのが無難です。\n",
        "\n",
        "SQL言語の設計思想は、欲しいデータが持っている関係をプログラムに伝えれば、それを満たすようなデータが返ってくる（効率的に処理する役目は言語の方にある）、というものです。これによって、利用者側がデータ処理について考えることが減り、本来やりたいことであるデータ解析にリソースを割くことができます（というのが理想です、やはり実際にはある程度は内部動作を知っておく必要があります）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StNsMT50sos7"
      },
      "source": [
        "pandasではqueryを用い、そこに条件式を書きました。SQLでは、\n",
        "\n",
        "```\n",
        "SELECT 列名1,... FROM テーブル名 [WHERE 条件式];\n",
        "```\n",
        "のようなSQL文で検索を行います。\n",
        "\n",
        "試しに実行してみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eCoTNfEwMfn"
      },
      "source": [
        "# cases列の指定\n",
        "a = %sql SELECT cases FROM eu;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErzToDCIm5y2"
      },
      "source": [
        "# cases列の取得\n",
        "%time res_cases = %sql SELECT cases from eu;\n",
        "display(len(res_cases))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ9mkMtVHXi1"
      },
      "source": [
        "# cases列の取得\n",
        "%time %sql SELECT COUNT(cases) from eu;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOprXlQg9_Lg"
      },
      "source": [
        "# cases < 60のレコードの取得 *は全ての列を意味\n",
        "%time res_cases = %sql SELECT * from eu WHERE cases < 60;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgN05Rh_p2cm"
      },
      "source": [
        "display(len(res_cases))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_mmkXUwHeYH"
      },
      "source": [
        "# cases < 60のレコードの取得 *は全ての列を意味\n",
        "%time res_cases = %sql SELECT * from eu WHERE cases == 58;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R_jb3NutatM"
      },
      "source": [
        "続けてもう少し大きいテーブルからの検索を実行してみます。\n",
        "\n",
        "そのためにまずテーブルの用意を行います。先のバルクインサートで用いたtuples_csvを200回使って200倍のレコードをもつテーブルを作成してみます。もっと効率よくダミーデータを作成する手法もあったりするようなのですが、colab上のSQLiteで検証できていないのでこの単純な方法でデータを水増しします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_aZkXV_tmm-"
      },
      "source": [
        "%%time\n",
        "\n",
        "con = sqlite3.connect('tmp.sqlite3')\n",
        "cur = con.cursor()\n",
        "\n",
        "cmd_create = ('CREATE TABLE IF NOT EXISTS eu2 ('\n",
        "  'dateRep TEXT,' \n",
        "  'day INTEGER,' \n",
        "  'month INTEGER,' \n",
        "  'year INTEGER,' \n",
        "  'cases INTEGER,' \n",
        "  'deaths INTEGER,' \n",
        "  'countriesAndTerritories TEXT,' \n",
        "  'geoId TEXT,' \n",
        "  'countryterritoryCode TEXT,'\n",
        "  'popData2019 TEXT,'\n",
        "  'continentExp TEXT,'\n",
        "  'Cumulative_number_for_14_days_of_COVID_19_cases_per_100000 REAL)')\n",
        "cur.execute(cmd_create)\n",
        "for i in range(0,200):\n",
        "  cur.executemany(\n",
        "      'INSERT INTO eu2 (dateRep,day,month,year,cases,deaths,countriesAndTerritories,geoId,countryterritoryCode,popData2019,continentExp,Cumulative_number_for_14_days_of_COVID_19_cases_per_100000) ' + \\\n",
        "      ' VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', tuples_csv\n",
        "  )\n",
        "\n",
        "con.commit()\n",
        "con.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQlThqqUy0iC"
      },
      "source": [
        "!ls -lha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8qC4E2KubnS"
      },
      "source": [
        "%sql SELECT COUNT(*) FROM eu2;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjyZsFgP16jR"
      },
      "source": [
        "%%time\n",
        "# cases < 60のレコードの取得 *は全ての列を意味　文字列の場合はLIKE演算子など\n",
        "res_cases = %sql SELECT cases from eu WHERE cases = 58;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXPmIquD18RQ"
      },
      "source": [
        "# cases列の取得\n",
        "# RDBMSの方で処理が済んでも　pythonに送り込むときに大きく遅延することもある\n",
        "#%time res_cases = %sql SELECT cases from eu;\n",
        "#display(len(res_cases))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uirDtwyHfZZ"
      },
      "source": [
        "#### インデックスを作成し検索のスピードアップを期待する\n",
        "pandasのインデックスは行のラベルという程度の意味でしたが、RDBのインデックスは文字通り索引のように検索を高速化させることに役立ちます（その仕組みについてはB-treeやT-treeについて調べてみてください）。\n",
        "\n",
        "検索方法によっては高速化の恩恵にあずかれないこともありますが、データベースが大規模になる程効果を発揮します。\n",
        "\n",
        "後からインデックスを追加することができますので、euテーブルのcases列にインデックスを追加してみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wfIPCx6IzKD"
      },
      "source": [
        "%%sql\n",
        "CREATE INDEX idx_cases ON eu (cases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF4nSVCsphDZ"
      },
      "source": [
        "%sql select * from sqlite_master;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8see9O0aMFJ0"
      },
      "source": [
        "# インデックスを消したい場合\n",
        "%sql DROP INDEX idx_cases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQzPpuenKuDN"
      },
      "source": [
        "%%time\n",
        "# cases < 60のレコードの取得 *は全ての列を意味\n",
        "res_cases = %sql SELECT cases from eu WHERE cases < 60;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShduV-JBqAlP"
      },
      "source": [
        "display(len(res_cases))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkPMVGUsZh-Y"
      },
      "source": [
        "# どのようにRDBMSがテーブルを検索しているかを知る\n",
        "%sql EXPLAIN QUERY PLAN SELECT cases from eu WHERE cases < 60;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTF_h-yuSqKd"
      },
      "source": [
        "### pandas (python) と何が違う？\n",
        "#### pandasでの処理が有利な点\n",
        "操作するデータは全てメインメモリ上に存在する。\n",
        "メインメモリへのアクセス速度はSSDやHDDへよりもはるかに高速である。\n",
        "データの全ての要素に対してアクセスするような処理で、かつそのデータが全てメモリに載るサイズで、かつデータを操作するイベントが自分の管理下にある（自分が処理している間に他人や他のプログラムがデータにアクセスしない）という条件であれば、RDBよりもpython上で処理した方が良いケースが多いと思われる。\n",
        "\n",
        "#### pandasでの処理が不利な点\n",
        "* メモリに載らないサイズのデータはそもそも扱えないため、サイズダウンするか分割して処理する必要がある。\n",
        "  * であればストレージの転送速度に拘束される。\n",
        "* SSDやHDDに比べてメモリは容量あたりの価格が高いうえ、物理的な上限もありHPC向けのサーバーでも8TB程度までである。\n",
        "* 現実的には数百GBを超えた時点でオンメモリ以外の方法を検討すべき（果たして本当に自分がやりたい処理はデータを全てメモリに乗せておく必要があるのか？）。\n",
        "* 実質的にそのプログラムしかデータにアクセスすることができない\n",
        "* データへのアクセスやデータ処理は全て自分でプログラミングする必要があり、操作効率は自身の実装に依存する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_7YnmsBVt6j"
      },
      "source": [
        "#### RDBが有利な点\n",
        "* データ操作の信頼性を保つような性質 (ACID特性) を考慮したシステムが実装されており、データ操作の整合性を取るために自分が余計なことに気を使わなくて良い\n",
        " * A, Atomicity (不可分性):\n",
        " * C, Consistency (一貫性): \n",
        " * I, Isolation (独立性): \n",
        " * D, Durability (永続性): \n",
        "* データやデータ操作の具体的な内容に深く踏み込まず、抽象的なまま扱える\n",
        "* どうやってデータを取得してくるのか、などデータ操作のためのプログラミングをしたり意識したりする必要がなく、どのようなデータが欲しいのかだけをSQL言語で記述すれば良い\n",
        "  （とはいえ、パフォーマンスを出すためにはある程度内部構造を知っておく必要がある）\n",
        "* 効率よくデータ操作できるように、RDBMSの方でオプティマイザがよしなに命令を解釈して実行してくれる\n",
        "* データへのアクセス規約が整理されており、それに対応していればどのようなアプリケーションからでもデータにアクセスできる。\n",
        "\n",
        "* メモリ上のデータは揮発するが、データを永続化し蓄積させることができる\n",
        "* HDDのような遅い記憶装置に最適な操作が提供されているほか、効率的な検索のための機能がある（インデックスなど）\n",
        "* そのためメモリに載せることができないようなサイズのデータ処理も可能\n",
        "\n",
        "\n",
        "#### RDBが不利な点\n",
        "* インデックスなどを活用しない場合、低速なストレージとデータをやり取りするため、高々数百MB, 数十GBのデータでは検索パフォーマンスはメモリ上で完結するpandasなどに比べて劣ると思われる\n",
        "* インデックスを全く使わないのであればRDBMSを採用する意味があまりない\n",
        "* 検索高速化のためにインデックスを作成すればするほど、データの更新や追記のコストが増大する（インデックスも更新する必要がある）\n",
        "* ほとんどの場合、RDBMSはクライアント/サーバー構成を取るので、別マシンあるいは同一マシンの中でサーバーを起動しておく必要がある (SQLiteは単独で動作可能、dockerやsingularityで建てることができるので難しくはない)\n",
        "* 基本操作だけならそれなりに平易とはいえ、SQL言語で記述する必要がある"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaIvWxbPRYTQ"
      },
      "source": [
        "## 具体的なデータを用いた練習\n",
        "\n",
        "讃岐先生の授業で用いたデータのインポートなども試してみてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52M5dNTcW7l8"
      },
      "source": [
        "日本語を含むファイルは開くときに文字コードの問題が起きる可能性がありますが、Mac, linuxなら\n",
        "```\n",
        "iconv -f cp932 -t utf8 入力ファイル名 > 新規出力ファイル名\n",
        "```\n",
        "などとして文字コードを変換するか、\n",
        "pd.read_csv(XXX, encoding='cp932')\n",
        "などとして解決を試みてください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uJsv4yKwnr2"
      },
      "source": [
        "インデックスを貼ってパフォーマンスを比較してみましょう。\n",
        "基本的にインデックスは、その列のデータ行が多いほど、その列のデータの種類が多い（カーディナリティが高い）ほど有効になる。\n",
        "\n",
        "例えば、入院か外来かの列にインデックスを張ってもあまり大した効果はないでしょう。\n",
        "\n",
        "インデックスのデメリットとしては、データの追加・更新が遅くなることがあります。\n",
        "それはインデックスの数が増えるほど顕著になりますが、それは更新のあった列などはインデックスを再計算することになるからです。\n",
        "Microsoft SQL serverのエンタープライズ向けなど高額な商用ソフトであれば軽減するための機能があったりします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMltEM4wEPW6"
      },
      "source": [
        "# いまどれほどメモリを消費しているか\n",
        "import sys\n",
        "\n",
        "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
        "print(\" ------------------------------------ \")\n",
        "for var_name in dir():\n",
        "    if not var_name.startswith(\"_\"):\n",
        "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rKZcenl9L3v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K1qAalB44jB"
      },
      "source": [
        "## (参考: NoSQL) グラフデータベース"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bJ7YUEYFXqs"
      },
      "source": [
        "Reactomeなどのパスウェイデータベースが身近。SQLクエリとはまた別のCypher Query Language (CQL) を用いる。RDBよりも汎用性に欠けるが特定の目的にはRDBよりもパフォーマンスを発揮する。"
      ]
    }
  ]
}